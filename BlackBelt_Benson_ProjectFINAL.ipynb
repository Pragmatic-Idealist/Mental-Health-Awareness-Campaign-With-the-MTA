{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BlackBelt Consulting\n",
    "\n",
    "## 1. Benson Project\n",
    "\n",
    "### 1.1 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:07.600990Z",
     "start_time": "2019-09-24T20:09:07.374324Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:08.181016Z",
     "start_time": "2019-09-24T20:09:08.178948Z"
    }
   },
   "outputs": [],
   "source": [
    "numdays = 353  # setting the number of days we want to take back 1 week from 06-29\n",
    "# We want data for this year (1st semester) and the last quarter of 2018 (so we can analyze the holiday's season)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:08.720003Z",
     "start_time": "2019-09-24T20:09:08.710945Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "datetime.datetime(2019, 6, 22, 8, 15, 27, 243860)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_time_str = '2019-06-22 08:15:27.243860'  # penultimate date of the range in the MTA website\n",
    "date_time_obj = datetime.datetime.strptime(date_time_str, '%Y-%m-%d %H:%M:%S.%f')  \n",
    "# transforming it to a date_time object\n",
    "date_time_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:09.451756Z",
     "start_time": "2019-09-24T20:09:09.447587Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[datetime.datetime(2019, 6, 22, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 6, 15, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 6, 8, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 6, 1, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 5, 25, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 5, 18, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 5, 11, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 5, 4, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 4, 27, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 4, 20, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 4, 13, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 4, 6, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 3, 30, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 3, 23, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 3, 16, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 3, 9, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 3, 2, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 2, 23, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 2, 16, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 2, 9, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 2, 2, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 1, 26, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 1, 19, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 1, 12, 8, 15, 27, 243860),\n",
       " datetime.datetime(2019, 1, 5, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 12, 29, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 12, 22, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 12, 15, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 12, 8, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 12, 1, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 11, 24, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 11, 17, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 11, 10, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 11, 3, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 10, 27, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 10, 20, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 10, 13, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 10, 6, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 9, 29, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 9, 22, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 9, 15, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 9, 8, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 9, 1, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 8, 25, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 8, 18, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 8, 11, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 8, 4, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 7, 28, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 7, 21, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 7, 14, 8, 15, 27, 243860),\n",
       " datetime.datetime(2018, 7, 7, 8, 15, 27, 243860)]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "date_list = [date_time_obj - datetime.timedelta(days=x) for x in range(0, numdays, 7)]\n",
    "# creating a list of dates that starts on the penultimate date and goes back the numdays we've set \n",
    "# (jumping 7 days each time)\n",
    "date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:10.272215Z",
     "start_time": "2019-09-24T20:09:10.266307Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['190622', '190615', '190608', '190601', '190525']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# transforming the dates into strings and putting in a list:\n",
    "\n",
    "url_dates = []\n",
    "\n",
    "for i in date_list:\n",
    "    year = str(i.year).replace(\"20\", \"\")\n",
    "    day = str(i.day)\n",
    "    if i.day < 10:\n",
    "        day = \"0\" + day\n",
    "    month = str(i.month)\n",
    "    if i.month < 10:\n",
    "        month = \"0\" + month\n",
    "    date_str = year + month + day\n",
    "    url_dates.append(date_str)\n",
    "\n",
    "url_dates[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-24T20:09:54.217950Z",
     "start_time": "2019-09-24T20:09:11.043068Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>C/A</th>\n",
       "      <th>UNIT</th>\n",
       "      <th>SCP</th>\n",
       "      <th>STATION</th>\n",
       "      <th>LINENAME</th>\n",
       "      <th>DIVISION</th>\n",
       "      <th>DATE</th>\n",
       "      <th>TIME</th>\n",
       "      <th>DESC</th>\n",
       "      <th>ENTRIES</th>\n",
       "      <th>EXITS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/22/2019</td>\n",
       "      <td>00:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7107725</td>\n",
       "      <td>2407457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/22/2019</td>\n",
       "      <td>04:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7107738</td>\n",
       "      <td>2407465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/22/2019</td>\n",
       "      <td>08:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7107761</td>\n",
       "      <td>2407491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/22/2019</td>\n",
       "      <td>12:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7107858</td>\n",
       "      <td>2407541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A002</td>\n",
       "      <td>R051</td>\n",
       "      <td>02-00-00</td>\n",
       "      <td>59 ST</td>\n",
       "      <td>NQR456W</td>\n",
       "      <td>BMT</td>\n",
       "      <td>06/22/2019</td>\n",
       "      <td>16:00:00</td>\n",
       "      <td>REGULAR</td>\n",
       "      <td>7108075</td>\n",
       "      <td>2407581</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    C/A  UNIT       SCP STATION LINENAME DIVISION        DATE      TIME  \\\n",
       "0  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/22/2019  00:00:00   \n",
       "1  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/22/2019  04:00:00   \n",
       "2  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/22/2019  08:00:00   \n",
       "3  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/22/2019  12:00:00   \n",
       "4  A002  R051  02-00-00   59 ST  NQR456W      BMT  06/22/2019  16:00:00   \n",
       "\n",
       "      DESC  ENTRIES  \\\n",
       "0  REGULAR  7107725   \n",
       "1  REGULAR  7107738   \n",
       "2  REGULAR  7107761   \n",
       "3  REGULAR  7107858   \n",
       "4  REGULAR  7108075   \n",
       "\n",
       "   EXITS                                                                 \n",
       "0                                            2407457                     \n",
       "1                                            2407465                     \n",
       "2                                            2407491                     \n",
       "3                                            2407541                     \n",
       "4                                            2407581                     "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# starting the dataframe with the last available date, which is june 29, 19:\n",
    "df = pd.read_csv(\"http://web.mta.info/developers/data/nyct/turnstile/turnstile_190629.txt\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:12.273Z"
    }
   },
   "outputs": [],
   "source": [
    "# concatenating each new date to the original dataframe:\n",
    "for url in url_dates:\n",
    "    df2 = pd.read_csv(\n",
    "        \"http://web.mta.info/developers/data/nyct/turnstile/turnstile_{}.txt\".format(url))\n",
    "    df = pd.concat([df, df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:12.780Z"
    }
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:13.087Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info(null_counts=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Data Treatment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:13.495Z"
    }
   },
   "outputs": [],
   "source": [
    "# First, let's transform the date and time columns into one column as a datetime object:\n",
    "df[\"DATE_TIME\"] = pd.to_datetime(df.DATE + \" \" + df.TIME, \n",
    "                                            format=\"%m/%d/%Y %H:%M:%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:13.655Z"
    }
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.156Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(df[\"DATE\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.237Z"
    }
   },
   "outputs": [],
   "source": [
    "df.columns = [column.strip() for column in df.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.322Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"TURNSTILE_ID\"] = df[\"C/A\"] + \" \" + df[\"UNIT\"] + \" \" + df[\"SCP\"] + \" \" + df[\"STATION\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.407Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.536Z"
    }
   },
   "outputs": [],
   "source": [
    "# Checking if there are readings that are duplicates\n",
    "(df\n",
    " .groupby([\"TURNSTILE_ID\", \"DATE_TIME\"])\n",
    " .ENTRIES.count()\n",
    " .reset_index()\n",
    " .sort_values(\"ENTRIES\", ascending=False)).head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.602Z"
    }
   },
   "outputs": [],
   "source": [
    "# On some days, we seem to have two entries for same time.  Let's take a look at a couple of examples:\n",
    "mask = ((df[\"TURNSTILE_ID\"] == \"N071 R013 00-00-00 34 ST-PENN STA\") &\n",
    "(df[\"DATE_TIME\"].dt.date == datetime.datetime(2019, 2, 27).date()))\n",
    "\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On this case, the RECOVR AUD seems to be the correct one since the entries and exits values are similar to the other readings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.753Z"
    }
   },
   "outputs": [],
   "source": [
    "# Other example:\n",
    "mask = ((df[\"TURNSTILE_ID\"] == \"R205A R014 04-02-01 FULTON ST\") &\n",
    "(df[\"DATE_TIME\"].dt.date == datetime.datetime(2018, 10, 21).date()))\n",
    "\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the other hand, in this case, the RECOVR AUD seems to be the wrong one (exits are much larger than the regular exits)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.914Z"
    }
   },
   "outputs": [],
   "source": [
    "# Since we can't map every one of these ocurrencies, we will remove all the duplicates and try to deal with the \n",
    "# aparent register errors in another way later:\n",
    "df.sort_values([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], \n",
    "                          inplace=True, ascending=False)\n",
    "df.drop_duplicates(subset=[\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:14.994Z"
    }
   },
   "outputs": [],
   "source": [
    "# No duplicate problems anymore:\n",
    "(df\n",
    " .groupby([\"C/A\", \"UNIT\", \"SCP\", \"STATION\", \"DATE_TIME\"])\n",
    " .ENTRIES.count()\n",
    " .reset_index()\n",
    " .sort_values(\"ENTRIES\", ascending=False)).head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.108Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For our data we are aiming only in the entries since we believe this is a better approach to gauge the number of people that may be focusing on the ads. \n",
    "\n",
    "We can see from the table above that the average of cumulative exits is much lower than the entries. This probably happens because many people don't use the turnstiles to exit a station.\n",
    "\n",
    "Also, people entering the stations are more likely to spend more time inside it than people exiting the station (waiting for the subway to arrive) and therefore, more chances to see the digital banners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.253Z"
    }
   },
   "outputs": [],
   "source": [
    "# Conclusion: Drop Exits and Desc Column.  To prevent errors in multiple run of cell, errors on drop is ignored\n",
    "df = df.drop([\"EXITS\", \"DESC\"], axis=1, errors=\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.331Z"
    }
   },
   "outputs": [],
   "source": [
    "df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.412Z"
    }
   },
   "outputs": [],
   "source": [
    "# There seems to be a problem with 23rd st station (and others) because different stations are named the same way:\n",
    "df[df[\"STATION\"] == \"23 ST\"].groupby([\"LINENAME\"]).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to solve this by changing only the name of the most important problematic stations (the ones that probably have a high foot traffic and may mess up our results) into \"station + line\".\n",
    "\n",
    "We'll do this for:\n",
    "- 23 st\n",
    "- 86 st\n",
    "- 96 st\n",
    "- 14 st\n",
    "- 125 st\n",
    "- Chambers St\n",
    "- 50 st\n",
    "- Canal St\n",
    "- 28 St\n",
    "- 72 St"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.576Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create a new dataframe with only the problematic stations and remove them from the original df.\n",
    "prob = ((df[\"STATION\"] == '14 ST') |\n",
    "(df[\"STATION\"] == '23 ST') | \n",
    "(df[\"STATION\"] == '86 ST') |\n",
    "(df[\"STATION\"] == '96 ST') |\n",
    "(df[\"STATION\"] == 'CANAL ST') |\n",
    "(df[\"STATION\"] == 'CHAMBERS ST') |\n",
    "(df[\"STATION\"] == '50 ST') |\n",
    "(df[\"STATION\"] == '28 ST') |\n",
    "(df[\"STATION\"] == '72 ST') |\n",
    "(df[\"STATION\"] == '125 ST'))\n",
    "df_prob = df[prob]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.662Z"
    }
   },
   "outputs": [],
   "source": [
    "df.drop(df[prob].index, inplace=True)\n",
    "sorted(df[\"STATION\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.744Z"
    }
   },
   "outputs": [],
   "source": [
    "# For this new df, let's change the STATION to STATION + LINENAME:\n",
    "\n",
    "df_prob[\"STATION\"] = df_prob[\"STATION\"] + \" \" + df_prob[\"LINENAME\"]\n",
    "df_prob.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.857Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now, let's concatenate this modified df into our original df to bring these stations back:\n",
    "df = pd.concat([df, df_prob], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:15.915Z"
    }
   },
   "outputs": [],
   "source": [
    "sorted(df[\"STATION\"].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's try to calculate the number of real entries in each period of time, since the original data only shows the cumulative entries registered up to that time:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.080Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.groupby([\"STATION\",\"TURNSTILE_ID\", \"LINENAME\", \"DATE\", \"TIME\", \"DATE_TIME\"],as_index=False).ENTRIES.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.164Z"
    }
   },
   "outputs": [],
   "source": [
    "df[[\"PREV_DATETIME\", \"PREV_ENTRIES\"]] = (df\n",
    "                                                       .groupby([\"STATION\", \"TURNSTILE_ID\", \"LINENAME\"])[\"DATE_TIME\", \"ENTRIES\"]\n",
    "                                                       .apply(lambda grp: grp.shift(1)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.247Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.348Z"
    }
   },
   "outputs": [],
   "source": [
    "# Drop the rows for the earliest date_time in the df\n",
    "df.dropna(subset=[\"PREV_DATETIME\"], axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another issue we found was the fact that the entries counters usually count up, eventually hit some number, and reset to 0. Also, sometimes they also count down as a reverse counter. To solve this, let's use the following function:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.501Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_daily_counts(row, max_counter):\n",
    "    counter = row[\"ENTRIES\"] - row[\"PREV_ENTRIES\"]\n",
    "    if counter < 0:\n",
    "        # Maybe counter is reversed?\n",
    "        counter = -counter\n",
    "    if counter > max_counter:\n",
    "        # Maybe counter was reset to 0? \n",
    "        print(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "        counter = min(row[\"ENTRIES\"], row[\"PREV_ENTRIES\"])\n",
    "    if counter > max_counter:\n",
    "        # Check it again to make sure we're not still giving a counter that's too big\n",
    "        return 0\n",
    "    return counter\n",
    "\n",
    "# If counter is > 10k, then the counter might have been reset.\n",
    "# 10k seems to be a reasonable number for maximum people entering in a period of hours.\n",
    "# Just set it to zero as different counters have different cycle limits\n",
    "df[\"REAL_ENTRIES\"] = df.apply(get_daily_counts, axis=1, max_counter=10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.580Z"
    }
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.684Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df[\"STATION\"] == \"GRD CNTRL-42 ST\"].sort_values(by=[\"REAL_ENTRIES\"],ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.766Z"
    }
   },
   "outputs": [],
   "source": [
    "df[\"WEEKDAY\"] = df[\"DATE_TIME\"].dt.weekday_name\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.871Z"
    }
   },
   "outputs": [],
   "source": [
    "df = df.assign(SESSION=pd.cut(df.DATE_TIME.dt.hour,[-1,5,11,17,23],labels=['Dawn','Morning','Afternoon','Evening']))\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:16.926Z"
    }
   },
   "outputs": [],
   "source": [
    "df[df[\"SESSION\"] == \"Evening\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.081Z"
    }
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()  # automatic seaborn settings\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.165Z"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df['REAL_ENTRIES']\n",
    "             [df['REAL_ENTRIES'] < 2000]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.249Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's first get the daily entries by station:\n",
    "stations_daily = \\\n",
    "    (df.groupby(['STATION','DATE',\"WEEKDAY\"])['REAL_ENTRIES'].sum()\n",
    "                 .reset_index())  \n",
    "\n",
    "stations_daily.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.337Z"
    }
   },
   "outputs": [],
   "source": [
    "# To discover the stations with more entries:\n",
    "stations = \\\n",
    "    (stations_daily.groupby(['STATION'])['REAL_ENTRIES'].sum()\n",
    "                   .reset_index()\n",
    "                   .sort_values(by='REAL_ENTRIES',ascending=False))\n",
    "\n",
    "stations.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.423Z"
    }
   },
   "outputs": [],
   "source": [
    "# To get top 15 stations by daily volume (sum across all days is a reasonable way to define this):\n",
    "top15_stations = \\\n",
    "    (stations_daily.groupby(['STATION'])['REAL_ENTRIES'].sum()\n",
    "                   .reset_index()\n",
    "                   .sort_values(by='REAL_ENTRIES',ascending=False) \n",
    "                   .STATION.head(15))\n",
    "\n",
    "top15_stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.503Z"
    }
   },
   "outputs": [],
   "source": [
    "# next create a new df that filters the stations daily data down to the top 15 stations\n",
    "stations_daily_top15 = \\\n",
    "    stations_daily[stations_daily['STATION'].isin(top15_stations)]\n",
    "stations_daily_top15.sort_values(by = 'REAL_ENTRIES')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.585Z"
    }
   },
   "outputs": [],
   "source": [
    "# use seaborn to create a boxplot by station\n",
    "sns.boxplot('REAL_ENTRIES', 'STATION', data=stations_daily_top15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.667Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_per_day = stations_daily_top15.groupby([\"STATION\"])[\"REAL_ENTRIES\"].mean()\n",
    "avg_per_day = avg_per_day.to_frame().reset_index()\n",
    "avg_per_day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.751Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,8))\n",
    "sns.barplot('STATION', 'REAL_ENTRIES', data=avg_per_day.sort_values(['REAL_ENTRIES'], ascending=False), palette =\"Blues_d\");\n",
    "\n",
    "plt.xticks(rotation = 'vertical');\n",
    "plt.ylabel(\"AVERAGE DAILY ENTRIES\", fontsize=14);\n",
    "plt.xlabel(\"STATION\", fontsize=14)\n",
    "plt.title(\"TOP 15 - HIGH FOOT TRAFFIC STATIONS\", fontsize=20);\n",
    "plt.savefig(\"plot1.svg\", bbox_inches='tight')\n",
    "plt.savefig(\"plot1.png\", bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.834Z"
    }
   },
   "outputs": [],
   "source": [
    "# Now let's try to find more patterns:\n",
    "\n",
    "entries_by_weekday = df.groupby(['STATION',\"WEEKDAY\"])['REAL_ENTRIES'].mean()\n",
    "entries_by_weekday"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:17.920Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_by_weekday = entries_by_weekday.unstack(level = -1).reset_index()\n",
    "entries_by_weekday.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.005Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_weekday_top15 = \\\n",
    "    entries_by_weekday[entries_by_weekday['STATION'].isin(top15_stations)]\n",
    "entries_weekday_top15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.088Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_weekday_top15 = entries_weekday_top15.groupby('STATION').mean()\n",
    "avg_weekday_top15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.170Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = avg_weekday_top15.columns.tolist()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.252Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = cols[1] + \" \" + cols[5] + \" \" + cols[6] + \" \" + cols[4] + \" \" + cols[0]+ \" \" + cols[2] + \" \" + cols[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.338Z"
    }
   },
   "outputs": [],
   "source": [
    "cols = cols.split()\n",
    "cols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.422Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_weekday_top15 = avg_weekday_top15[cols]\n",
    "\n",
    "avg_weekday_top15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.506Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(avg_weekday_top15,annot=False, cmap=\"Reds\");\n",
    "plt.xlabel(\"WEEKDAY\", fontsize=14);\n",
    "plt.ylabel(\"STATION\", fontsize=14);\n",
    "plt.title(\"AVERAGE ENTRIES PER WEEKDAY\", fontsize=20)\n",
    "plt.savefig(\"plot2.svg\")\n",
    "plt.savefig(\"plot2.png\")\n",
    "# plt.xticks(ticks=[0,1,2,3,4,5,6], labels=[0,1,2,3,4,5,6]);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.587Z"
    }
   },
   "outputs": [],
   "source": [
    "top_15_stations = df[df['STATION'].isin(top15_stations)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.674Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_by_session = top_15_stations.groupby(['STATION',\"SESSION\"])['REAL_ENTRIES'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.759Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_by_session = avg_by_session.unstack(level = -1)\n",
    "avg_by_session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.839Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(avg_by_session,annot=False, cmap=\"Greens\");\n",
    "plt.xlabel(\"TIME OF DAY\", fontsize=14);\n",
    "plt.ylabel(\"STATION\", fontsize=14);\n",
    "plt.title(\"AVERAGE ENTRIES PER TIME OF DAY\", fontsize=20)\n",
    "plt.savefig(\"plot3.svg\")\n",
    "plt.savefig(\"plot3.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:18.922Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15 = top_15_stations.groupby([\"STATION\", \"DATE_TIME\"])[\"REAL_ENTRIES\"].sum()\n",
    "daily_entries_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.006Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15 = daily_entries_15.to_frame().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.093Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15['DATE'] = daily_entries_15['DATE_TIME'].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.177Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15 = daily_entries_15.groupby([\"STATION\", \"DATE\"])[\"REAL_ENTRIES\"].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.261Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15 = daily_entries_15.to_frame().reset_index()\n",
    "daily_entries_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.343Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15[\"DATE\"] = pd.to_datetime(daily_entries_15[\"DATE\"]) \n",
    "daily_entries_15.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.427Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15[\"MONTH\"] = daily_entries_15[\"DATE\"].dt.month\n",
    "daily_entries_15.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.509Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.593Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15[\"SEASON\"] = \"NS\"\n",
    "\n",
    "# Let's create a new dataframe with only the problematic stations and remove them from the original df.\n",
    "summer = ((daily_entries_15[\"MONTH\"] == 6) |\n",
    "(daily_entries_15[\"MONTH\"] == 7) | \n",
    "(daily_entries_15[\"MONTH\"] == 8))\n",
    "\n",
    "daily_entries_15.loc[summer,\"SEASON\"] = \"Summer\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.675Z"
    }
   },
   "outputs": [],
   "source": [
    "winter = ((daily_entries_15[\"MONTH\"] == 12) |\n",
    "(daily_entries_15[\"MONTH\"] == 1) | \n",
    "(daily_entries_15[\"MONTH\"] == 2))\n",
    "\n",
    "daily_entries_15.loc[winter,\"SEASON\"] = \"Winter\"\n",
    "\n",
    "fall = ((daily_entries_15[\"MONTH\"] == 9) |\n",
    "(daily_entries_15[\"MONTH\"] == 10) | \n",
    "(daily_entries_15[\"MONTH\"] == 11))\n",
    "\n",
    "daily_entries_15.loc[fall,\"SEASON\"] = \"Fall\"\n",
    "\n",
    "spring = ((daily_entries_15[\"MONTH\"] == 3) |\n",
    "(daily_entries_15[\"MONTH\"] == 4) | \n",
    "(daily_entries_15[\"MONTH\"] == 5))\n",
    "\n",
    "daily_entries_15.loc[spring,\"SEASON\"] = \"Spring\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.760Z"
    }
   },
   "outputs": [],
   "source": [
    "daily_entries_15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.842Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_entries_day = daily_entries_15.groupby(\"DATE\")[\"REAL_ENTRIES\"].mean()\n",
    "avg_entries_day = avg_entries_day.to_frame().reset_index()\n",
    "avg_entries_day.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:19.927Z"
    }
   },
   "outputs": [],
   "source": [
    "avg_entries_day[\"COLUMN\"] = \"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.011Z"
    }
   },
   "outputs": [],
   "source": [
    "seasonal = avg_entries_day.set_index('DATE').groupby('COLUMN')['REAL_ENTRIES'].rolling(30).mean()\n",
    "seasonal = seasonal.to_frame().reset_index()\n",
    "seasonal.drop([\"COLUMN\"],axis=1,inplace=True)\n",
    "seasonal.dropna(axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.095Z"
    }
   },
   "outputs": [],
   "source": [
    "seasonal.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.176Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "plt.plot(seasonal['DATE'],seasonal['REAL_ENTRIES'])\n",
    "plt.xlabel(\"DATE\", fontsize=14);\n",
    "plt.ylabel(\"AVERAGE ENTRIES\", fontsize=14);\n",
    "plt.title(\"TOP 15 STATIONS (HIGH FOOT TRAFFIC)\\nAVERAGE ENTRIES PER DAY - 30 DAYS ROLLING\", fontsize=20)\n",
    "plt.savefig(\"plot4.svg\")\n",
    "plt.savefig(\"plot4.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.260Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_month = daily_entries_15.groupby([\"STATION\", \"MONTH\"])[\"REAL_ENTRIES\"].mean()\n",
    "entries_per_month = entries_per_month.to_frame()\n",
    "entries_per_month = entries_per_month.unstack(-1)\n",
    "entries_per_month.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.342Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_month = entries_per_month.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.428Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_month = (100. * entries_per_month / entries_per_month.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.514Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_month = entries_per_month.transpose()\n",
    "entries_per_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.592Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(entries_per_month,annot=False, cmap=\"Greens\");\n",
    "plt.xlabel(\"MONTH\", fontsize=14);\n",
    "plt.ylabel(\"STATION\", fontsize=14);\n",
    "plt.title(\"AVERAGE ENTRIES PER MONTH\", fontsize=20)\n",
    "plt.savefig(\"plot5.svg\")\n",
    "plt.savefig(\"plot5.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.677Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_season = daily_entries_15.groupby([\"STATION\", \"SEASON\"])[\"REAL_ENTRIES\"].mean()\n",
    "entries_per_season = entries_per_season.to_frame()\n",
    "entries_per_season = entries_per_season.unstack(-1)\n",
    "entries_per_season.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.760Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_season = entries_per_season.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.844Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_season = (100. * entries_per_season / entries_per_season.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:20.927Z"
    }
   },
   "outputs": [],
   "source": [
    "entries_per_season = entries_per_season.transpose()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:21.011Z"
    }
   },
   "outputs": [],
   "source": [
    "# entries_per_season[()'REAL_ENTRIES', 'Winter'), ('REAL_ENTRIES', 'Spring'), ('REAL_ENTRIES', 'Summer'), ('REAL_ENTRIES', 'Fall')]\n",
    "entries_per_season = entries_per_season['REAL_ENTRIES'][[\"Winter\", 'Spring', 'Summer', 'Fall']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-24T20:09:21.095Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize = (15,10))\n",
    "sns.heatmap(entries_per_season,annot=False, cmap=\"YlOrRd\");\n",
    "plt.xlabel(\"SEASON\", fontsize=14);\n",
    "plt.ylabel(\"STATION\", fontsize=14);\n",
    "plt.title(\"AVERAGE ENTRIES BY SEASON (AS % OF THE WHOLE YEAR)\", fontsize=20)\n",
    "plt.savefig(\"plot6.svg\")\n",
    "plt.savefig(\"plot6.png\")\n",
    "# plt.xticks(ticks=[0,1,2,3,4,5,6], labels=[0,1,2,3,4,5,6]);\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
